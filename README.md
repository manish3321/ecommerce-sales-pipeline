E-Commerce Sales Analytics Pipeline


ğŸš€ Overview

The Global E-Commerce Sales Analytics Pipeline is a real-world, production-ready ETL and analytics project built using open-source tools like Apache Airflow, PostgreSQL, Docker, and Tableau. The goal of this project is to simulate an end-to-end data engineering workflow: from ingesting data via APIs to transforming, storing, and visualizing it to extract actionable business insights.

This project demonstrates how modern data teams process raw data into business-ready insights and present it through intuitive dashboards that drive decision-making.


ğŸ“ˆ Project Objectives

                Build a scalable ETL pipeline for global e-commerce datasets
                
                Automate extraction and loading of product, user, and cart data
                
                Store cleaned data into a PostgreSQL data warehouse
                
                Visualize key metrics like sales trends, user engagement, and cart patterns in Tableau
                
                Use Docker for easy reproducibility and deployment
                
                Showcase the pipeline as a portfolio project for Data Engineering and BI roles

ğŸ“Š Tech Stack & Tools
                
                Tool
                
                Purpose
                
                Python
                
                Core ETL scripting
                
                Apache Airflow
                
                Workflow orchestration (scheduling ETL DAGs)
                
                PostgreSQL
                
                Data warehouse backend
                
                Docker & Docker Compose
                
                Containerizing the entire pipeline
                
                Tableau Public
                
                Data visualization and dashboarding
                
                Git & GitHub
                
                Version control and portfolio deployment
                
                ğŸ›€ Architecture Diagram

Data Flow:

Public API â” Python Scripts â” Airflow DAG â” PostgreSQL â” Tableau Dashboards

Extraction: Python scripts fetch JSON data from an e-commerce public API

Loading: The data is inserted into a PostgreSQL database using psycopg

Scheduling: Airflow DAG orchestrates daily data ingestion

Visualization: Tableau connects directly to PostgreSQL and visualizes KPIs

ğŸ“ Project Structure

Ecommerce/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ ecommerce_etl_dag.py        # Airflow DAG definition
â”‚
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ loaders/
â”‚       â”œâ”€â”€ load_products.py            # Fetch & load product data
â”‚       â”œâ”€â”€ load_users.py               # Fetch & load user data
â”‚       â””â”€â”€ load_carts.py               # Fetch & load cart data
â”‚
â”œâ”€â”€ transformations/                   # (Optional) Transform logic with pandas
â”œâ”€â”€ Data/                               # Storage for raw or intermediate files
â”œâ”€â”€ requirements.txt                    # Python package dependencies
â”œâ”€â”€ docker-compose.yml                 # Container orchestration
â””â”€â”€ README.md

ğŸ”„ Data Pipeline Components

1. Data Extraction

          Uses Python's requests module to fetch data from the FakeStore API.
          
          load_products.py, load_users.py, and load_carts.py parse and validate this data.

2. Data Loading

          Transforms the JSON data into structured SQL inserts
          
          PostgreSQL stores the data into relational tables: products, users, carts

3. DAG Scheduling

          DAG ecommerce_etl_dag.py runs every 24 hours
          
          Tasks are defined using PythonOperator
          
          Dependencies set as: load_products >> load_users >> load_carts

4. Data Warehouse (PostgreSQL)

          Hosted via Docker and exposed on port 5433
          
          Database: retaildb
          
          Schema normalization used to ensure relational integrity

ğŸ“† Airflow DAG Snapshot

          with DAG(
              'ecommerce_etl_dag',
              schedule_interval='@daily',
              default_args=default_args,
              catchup=False,
              description='ETL DAG for E-commerce API data'
          ) as dag:
              t1 = PythonOperator(task_id='load_products', python_callable=load_products)
              t2 = PythonOperator(task_id='load_users', python_callable=load_users)
              t3 = PythonOperator(task_id='load_carts', python_callable=load_carts)
          
              t1 >> t2 >> t3

ğŸ“Š Tableau Dashboard

The Tableau dashboard is built by directly connecting to the PostgreSQL database running in Docker.

Key Insights Visualized

          â‚¹ Total Sales by Product Category
          
          ğŸ‘¨ï¸ Top Purchasing Users
          
          ğŸ›’ Cart Abandonment Trends
          
          ğŸŒ Sales by Country (via user address)
          
          ğŸ“… Daily & Weekly Revenue Trends
          
          ğŸ”— Link to Tableau Public Dashboard

ğŸšª How to Run This Project

              Prerequisites
              
              Docker & Docker Compose
              
              Git
              
              Tableau Desktop or Tableau Public

Steps

              git clone https://github.com/manish3321/ecommerce-sales-pipeline.git
              cd ecommerce-sales-pipeline
              docker-compose up --build

              Access Airflow UI: http://localhost:8080
              
              Username: admin  Password: admin
              
              Trigger ecommerce_etl_dag
              
              Connect Tableau to PostgreSQL (host: ****, port: 5433, db: *****)

ğŸš€ Achievements

              End-to-end working data pipeline in Docker
              
              Airflow DAGs with proper task dependencies
              
              Custom loader scripts with error handling
              
              Tableau dashboard powered by live PostgreSQL data
              
              Clean GitHub project with production potential



ğŸšª Security & Best Practices

            Environment variables are managed securely in Docker Compose
            
            Airflow secrets and database passwords externalized (optional: .env)
            
            Retry logic in DAGs and subprocesses



ğŸ“• Future Enhancements

          Add Airflow email alerts and logging enhancements
          
          Integrate dbt for transformation layer
          
          Host PostgreSQL and Airflow on cloud (AWS/GCP/Azure)
          


ğŸ‘¨â€ğŸ’¼ About the Author

Manish Bhattarai
Data Engineer | Pythonista | BI EnthusiastGitHub: @manish3321LinkedIn: https://www.linkedin.com/in/manish-bhattarai-347497148/


